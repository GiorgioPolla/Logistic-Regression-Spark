{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralised Logistic Regression - Giorgio Polla\n",
    "\n",
    "Logistic Regression Classifier implementation.   \n",
    "Centralised version, without cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    read_data = pd.read_csv(filename, header=None)\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_file(\"../data/spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(df):\n",
    "    return df.iloc[:, 0:-1].values\n",
    "\n",
    "def get_y(df):\n",
    "    return df.iloc[:, -1].values\n",
    "\n",
    "def standardize(df):\n",
    "    x = get_x(df)\n",
    "    df_y = df.iloc[:, -1]\n",
    "    \n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    x_scaled = standard_scaler.fit_transform(x)\n",
    "    \n",
    "    df_x = pd.DataFrame(x_scaled)\n",
    "    df_scaled = df_x.join(df_y)\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>0.330885</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.624007</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.045247</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>-0.008724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.345359</td>\n",
       "      <td>0.051909</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.672399</td>\n",
       "      <td>0.244743</td>\n",
       "      <td>-0.088010</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.086711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.026007</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.126203</td>\n",
       "      <td>0.423783</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.250563</td>\n",
       "      <td>1.228324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.145921</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.851723</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.364846</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>0.193644</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>1.974017</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117376</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.440053</td>\n",
       "      <td>-0.079754</td>\n",
       "      <td>0.145921</td>\n",
       "      <td>2.221106</td>\n",
       "      <td>3.258733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.161934</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.014910</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.164387</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2       3         4         5         6   \\\n",
       "0 -0.342434  0.330885  0.712859 -0.0469  0.011565 -0.350266 -0.291794   \n",
       "1  0.345359  0.051909  0.435130 -0.0469 -0.256117  0.672399  0.244743   \n",
       "2 -0.145921 -0.165072  0.851723 -0.0469  1.364846  0.343685  0.193644   \n",
       "3 -0.342434 -0.165072 -0.556761 -0.0469  0.472573 -0.350266  0.500237   \n",
       "4 -0.342434 -0.165072 -0.556761 -0.0469  0.472573 -0.350266  0.500237   \n",
       "\n",
       "         7         8         9   ...        48        49        50        51  \\\n",
       "0 -0.262562 -0.323302 -0.371364  ... -0.158453 -0.514307 -0.155198  0.624007   \n",
       "1 -0.088010 -0.323302  1.086711  ... -0.158453 -0.026007 -0.155198  0.126203   \n",
       "2  0.036670  1.974017  0.016422  ... -0.117376  0.014684 -0.155198  0.008496   \n",
       "3  1.308402  0.789462  0.605857  ... -0.158453 -0.007511 -0.155198 -0.161934   \n",
       "4  1.308402  0.789462  0.605857  ... -0.158453 -0.014910 -0.155198 -0.164387   \n",
       "\n",
       "         52        53        54        55        56  57  \n",
       "0 -0.308355 -0.103048 -0.045247  0.045298 -0.008724   1  \n",
       "1  0.423783  0.008763 -0.002443  0.250563  1.228324   1  \n",
       "2  0.440053 -0.079754  0.145921  2.221106  3.258733   1  \n",
       "3 -0.308355 -0.103048 -0.052150 -0.062466 -0.152222   1  \n",
       "4 -0.308355 -0.103048 -0.052150 -0.062466 -0.152222   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = standardize(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict_probability(x, b, w):\n",
    "    z = np.dot(x, w) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "def loss(p, y, w, lambda_reg):\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    error_loss = -np.average(\n",
    "        y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon)\n",
    "    )\n",
    "    reg_loss = lambda_reg * np.sum(np.square(w)) / (2 * y.size)\n",
    "    \n",
    "    return error_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, iterations=300, learning_rate=10, lambda_reg=1):\n",
    "    x = get_x(df)\n",
    "    y = get_y(df)\n",
    "    \n",
    "    b = 0\n",
    "    w = np.zeros(x.shape[1])\n",
    "    \n",
    "    loss_history = [[],[]]\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        pred = predict_probability(x, b, w)\n",
    "        \n",
    "        gradient_w = np.dot(x.T, (pred - y))\n",
    "        gradient_b = np.average(pred - y)\n",
    "        regularization = lambda_reg * w\n",
    "        \n",
    "        b -= learning_rate * gradient_b\n",
    "        w -= learning_rate * (gradient_w + regularization) / y.size\n",
    "        \n",
    "        if it % 10 == 0 or it == iterations - 1:\n",
    "            temp_loss = loss(pred, y, w, lambda_reg)\n",
    "            loss_history[0].append(temp_loss)\n",
    "            loss_history[1].append(it)\n",
    "            \n",
    "            if it % (iterations / 5) == 0:\n",
    "                print(\"It. %4d\\t|\\tLoss: %0.4f\" %  (it, temp_loss))\n",
    "        \n",
    "    return b, w, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It.    0\t|\tLoss: 0.6981\n",
      "It.   60\t|\tLoss: 0.2132\n",
      "It.  120\t|\tLoss: 0.2116\n",
      "It.  180\t|\tLoss: 0.2112\n",
      "It.  240\t|\tLoss: 0.2110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcaa7867810>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b, w, loss_history = train(df)\n",
    "len(w)\n",
    "\n",
    "plt.plot(np.asarray(loss_history[1]), np.asarray(loss_history[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, b, w, threshold=0.5):\n",
    "    prob = predict_probability(x, b, w)\n",
    "    return prob >= threshold\n",
    "\n",
    "def accuracy(df, b, w, threshold=0.5):\n",
    "    x = get_x(df)\n",
    "    y = get_y(df)\n",
    "    pred = predict(x, b, w, threshold=threshold)\n",
    "    \n",
    "    accuracy = np.average(pred == y)\n",
    "    print(\"Accuracy: %0.4f\" % accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9302325581395349"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(df, b, w, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Accuracy sklearn: 0.9302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = get_x(df)\n",
    "y = get_y(df)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    verbose=1, \n",
    "    max_iter=9, \n",
    "    solver='liblinear'\n",
    ")\n",
    "lr_model.fit(x, y)\n",
    "\n",
    "print('Accuracy sklearn: {:.4f}'.format(lr_model.score(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
