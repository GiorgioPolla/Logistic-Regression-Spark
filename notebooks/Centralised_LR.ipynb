{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralised Logistic Regression - Giorgio Polla\n",
    "\n",
    "Logistic Regression Classifier implementation.   \n",
    "Centralised version, without cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    read_data = pd.read_csv(filename, header=None)\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_file(\"../data/spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(df):\n",
    "    return df.iloc[:, 0:-1].values\n",
    "\n",
    "def get_y(df):\n",
    "    return df.iloc[:, -1].values\n",
    "\n",
    "def standardize(df):\n",
    "    x = get_x(df)\n",
    "    df_y = df.iloc[:, -1]\n",
    "    \n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    x_scaled = standard_scaler.fit_transform(x)\n",
    "    \n",
    "    df_x = pd.DataFrame(x_scaled)\n",
    "    df_scaled = df_x.join(df_y)\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>0.330885</td>\n",
       "      <td>0.712859</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.624007</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.045247</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>-0.008724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.345359</td>\n",
       "      <td>0.051909</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.672399</td>\n",
       "      <td>0.244743</td>\n",
       "      <td>-0.088010</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.086711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.026007</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.126203</td>\n",
       "      <td>0.423783</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.250563</td>\n",
       "      <td>1.228324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.145921</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.851723</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>1.364846</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>0.193644</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>1.974017</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117376</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.440053</td>\n",
       "      <td>-0.079754</td>\n",
       "      <td>0.145921</td>\n",
       "      <td>2.221106</td>\n",
       "      <td>3.258733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.161934</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.472573</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.605857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.014910</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.164387</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2       3         4         5         6   \\\n",
       "0 -0.342434  0.330885  0.712859 -0.0469  0.011565 -0.350266 -0.291794   \n",
       "1  0.345359  0.051909  0.435130 -0.0469 -0.256117  0.672399  0.244743   \n",
       "2 -0.145921 -0.165072  0.851723 -0.0469  1.364846  0.343685  0.193644   \n",
       "3 -0.342434 -0.165072 -0.556761 -0.0469  0.472573 -0.350266  0.500237   \n",
       "4 -0.342434 -0.165072 -0.556761 -0.0469  0.472573 -0.350266  0.500237   \n",
       "\n",
       "         7         8         9   ...        48        49        50        51  \\\n",
       "0 -0.262562 -0.323302 -0.371364  ... -0.158453 -0.514307 -0.155198  0.624007   \n",
       "1 -0.088010 -0.323302  1.086711  ... -0.158453 -0.026007 -0.155198  0.126203   \n",
       "2  0.036670  1.974017  0.016422  ... -0.117376  0.014684 -0.155198  0.008496   \n",
       "3  1.308402  0.789462  0.605857  ... -0.158453 -0.007511 -0.155198 -0.161934   \n",
       "4  1.308402  0.789462  0.605857  ... -0.158453 -0.014910 -0.155198 -0.164387   \n",
       "\n",
       "         52        53        54        55        56  57  \n",
       "0 -0.308355 -0.103048 -0.045247  0.045298 -0.008724   1  \n",
       "1  0.423783  0.008763 -0.002443  0.250563  1.228324   1  \n",
       "2  0.440053 -0.079754  0.145921  2.221106  3.258733   1  \n",
       "3 -0.308355 -0.103048 -0.052150 -0.062466 -0.152222   1  \n",
       "4 -0.308355 -0.103048 -0.052150 -0.062466 -0.152222   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = standardize(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict_probability(x, b, w):\n",
    "    z = np.dot(x, w) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "def loss(p, y, w, lambda_reg):\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    error_loss = -np.average(\n",
    "        y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon)\n",
    "    )\n",
    "    reg_loss = lambda_reg * np.sum(np.square(w)) / (2 * y.size)\n",
    "    \n",
    "    return error_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, iterations=20, learning_rate=1, lambda_reg=1):\n",
    "    x = get_x(df)\n",
    "    y = get_y(df)\n",
    "    \n",
    "    b = 0\n",
    "    w = np.zeros(x.shape[1])\n",
    "    \n",
    "    loss_history = [[],[]]\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        pred = predict_probability(x, b, w)\n",
    "        \n",
    "        gradient_w = np.dot(x.T, (pred - y))\n",
    "        gradient_b = np.average(pred - y)\n",
    "        regularization = lambda_reg * w\n",
    "        \n",
    "        b -= learning_rate * gradient_b\n",
    "        w -= learning_rate * (gradient_w + regularization) / y.size\n",
    "            \n",
    "        if it % (iterations / 5) == 0:\n",
    "            print(\"It. %4d\\t|\\tLoss: %0.4f\" %  (it, loss(pred, y, w, lambda_reg)))\n",
    "        \n",
    "        if it % 10 == 0 or it == iterations - 1:\n",
    "            temp_loss = loss(pred, y, w, lambda_reg)\n",
    "            loss_history[0].append(temp_loss)\n",
    "            loss_history[1].append(it)\n",
    "        \n",
    "    return b, w, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It.    0\t|\tLoss: 0.6932\n",
      "It.    4\t|\tLoss: 0.3232\n",
      "It.    8\t|\tLoss: 0.2905\n",
      "It.   12\t|\tLoss: 0.2754\n",
      "It.   16\t|\tLoss: 0.2664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa894a361d0>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b, w, loss_history = train(df)\n",
    "\n",
    "plt.plot(np.asarray(loss_history[1]), np.asarray(loss_history[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, b, w, threshold=0.5):\n",
    "    prob = predict_probability(x, b, w)\n",
    "    return prob >= threshold\n",
    "\n",
    "def accuracy(df, b, w, threshold=0.5):\n",
    "    x = get_x(df)\n",
    "    y = get_y(df)\n",
    "    pred = predict(x, b, w, threshold=threshold)\n",
    "    \n",
    "    accuracy = np.average(pred == y)\n",
    "    print(\"Accuracy: %0.4f\" % accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9141490980221691"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(df, b, w, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Accuracy sklearn: 0.9302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = get_x(df)\n",
    "y = get_y(df)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    verbose=1, \n",
    "    max_iter=9, \n",
    "    solver='liblinear'\n",
    ")\n",
    "lr_model.fit(x, y)\n",
    "\n",
    "print('Accuracy sklearn: {:.4f}'.format(lr_model.score(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
