{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "FILE_PATH = '../data/spam.txt'\n",
    "N_WORKERS = 8\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1b8320561006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "\n",
    "config = pyspark.SparkConf().setMaster('local[*]')\n",
    "sc = pyspark.SparkContext(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    rdd = sc.textFile(file_path)\n",
    "    \n",
    "    rdd = rdd.map(\n",
    "        lambda x: (\n",
    "            [float(el) for el in x[:-1].split()],\n",
    "            int(x[-1])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = read_file(FILE_PATH)\n",
    "rdd.take(1)[0][0][56] # test -> 278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(rdd):\n",
    "    mean = []\n",
    "    std_dev = []\n",
    "    \n",
    "    col_sum = rdd.map(\n",
    "        lambda x:\n",
    "            x[0]\n",
    "    ).reduce(\n",
    "        lambda x, y:\n",
    "            [sum(el) for el in zip(x, y)]\n",
    "    )\n",
    "    mean = np.divide(col_sum, rdd.count())\n",
    "    \n",
    "    variance = rdd.map(\n",
    "        lambda x: np.square(x[0] - mean)\n",
    "    ).reduce(\n",
    "        lambda x, y:\n",
    "            [sum(el) for el in zip(x, y)]\n",
    "    )\n",
    "    \n",
    "    std_dev = np.sqrt(np.divide(variance, rdd.count()))\n",
    "    \n",
    "    rdd = rdd.map(\n",
    "        lambda x: (\n",
    "            np.divide((x[0] - mean), std_dev),\n",
    "            x[1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = standardise(rdd)\n",
    "rdd.take(1)[0][0][56] # test -> -0.008724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def predict_probability(x, b, w):\n",
    "    z = np.dot(x, w) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "\n",
    "def cross_entropy(p, y):    \n",
    "    if y == 1:\n",
    "        error_loss = -np.log(p + EPSILON)\n",
    "    else:\n",
    "        error_loss = -np.log(1 - p + EPSILON)\n",
    "    \n",
    "    return error_loss\n",
    "\n",
    "def gradient_(rdd, b, w):\n",
    "    gradient_b, gradient_w, loss = rdd.map(\n",
    "        lambda x: (\n",
    "            predict_probability(x[0], b, w) - x[1],\n",
    "            np.multiply(\n",
    "                predict_probability(x[0], b, w) - x[1],\n",
    "                x[0]\n",
    "            ),\n",
    "            cross_entropy(\n",
    "                predict_probability(x[0], b, w),\n",
    "                x[1]\n",
    "            )\n",
    "        )\n",
    "    ).reduce(\n",
    "        lambda x, y: (\n",
    "            x[0] + y[0],\n",
    "            [sum(el) for el in zip(x[1], y[1])],\n",
    "            x[2] + y[2]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return gradient_b, gradient_w, loss\n",
    "\n",
    "def gradient(rdd, b, w):\n",
    "    gradient_b, gradient_w, loss = rdd.map(\n",
    "        lambda x: (\n",
    "            x[0],\n",
    "            x[1],\n",
    "            predict_probability(x[0], b, w),\n",
    "        )\n",
    "    ).map(\n",
    "        lambda x: (\n",
    "            x[2] - x[1],\n",
    "            np.multiply(\n",
    "                x[2] - x[1],\n",
    "                x[0]\n",
    "            ),\n",
    "            cross_entropy(\n",
    "                x[2],\n",
    "                x[1]\n",
    "            )\n",
    "        )\n",
    "    ).reduce(\n",
    "        lambda x, y: (\n",
    "            x[0] + y[0],\n",
    "            [sum(el) for el in zip(x[1], y[1])],\n",
    "            x[2] + y[2]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return gradient_b, gradient_w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rdd, iters=50, learning_rate=10, lambda_reg=1):\n",
    "    n_rows = rdd.count()\n",
    "    n_features = len(rdd.first()[0])\n",
    "    b = 0\n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for it in range(iters):\n",
    "        gradient_b, gradient_w, err_loss = gradient(rdd, b, w)\n",
    "        \n",
    "        reg_loss = lambda_reg * np.sum(np.square(w)) / 2\n",
    "        regularization = lambda_reg * w\n",
    "        \n",
    "        b -= learning_rate * gradient_b / n_rows\n",
    "        w -= learning_rate * (gradient_w + regularization) / n_rows\n",
    "        \n",
    "        total_loss = (err_loss + reg_loss) / n_rows\n",
    "        loss_history.append(total_loss)\n",
    "        \n",
    "        if it % (iters / 5) == 0:\n",
    "            print(\"It. %4d\\t|\\tLoss: %0.4f\\t|\\tTime: %0.2f s\" %  \n",
    "                  (it, total_loss, (time.time() - start_time)))\n",
    "    total_time = time.time() - start_time\n",
    "    print('\\nTotal time: %0.2f s\\nIters frequency: %0.2f Hz' % (total_time, iters / total_time))\n",
    "    \n",
    "    return b, w, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, w, loss_history = train(rdd)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(np.asarray(loss_history))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(prob, y, threshold=0.5):\n",
    "    if (prob >= threshold) == y:\n",
    "        return 'tp' if y==1 else 'tn'\n",
    "    else:\n",
    "        return 'fn' if y==1 else 'fp'\n",
    "            \n",
    "def evaluate(rdd, b, w, loss_history):\n",
    "    \n",
    "    rdd_pred = rdd.map(\n",
    "        lambda x: (\n",
    "            predict_probability(x[0], b, w),\n",
    "            x[1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    predictions = rdd_pred.map(\n",
    "        lambda x:\n",
    "            x[0]\n",
    "    ).collect()\n",
    "    \n",
    "    corrects = rdd.map(\n",
    "        lambda x:\n",
    "            x[1]\n",
    "    ).collect()\n",
    "    \n",
    "    c_m = rdd_pred.map(\n",
    "        lambda x: (\n",
    "            classify(x[0], x[1]),\n",
    "            1\n",
    "        )\n",
    "    ).reduceByKey(\n",
    "        lambda x, y:\n",
    "            x+y\n",
    "    )\n",
    "    c_m = dict(c_m.collect())\n",
    "    \n",
    "    roc1, roc2, _ = roc_curve(corrects, predictions)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['Confusion Matrix'] = c_m\n",
    "    \n",
    "    results['Accuracy'] = (c_m['tp'] + c_m['tn']) \\\n",
    "        / (c_m['tp'] + c_m['tn'] + c_m['fp'] + c_m['fn'])\n",
    "    \n",
    "    results['Precision'] = c_m['tp'] / (c_m['tp'] + c_m['fp'])\n",
    "    \n",
    "    results['Recall'] = c_m['tp'] / (c_m['tp'] + c_m['fn'])\n",
    "    \n",
    "    results['Specificity'] = c_m['tn'] / (c_m['tn'] + c_m['fp'])\n",
    "    \n",
    "    results['F1-Score'] = 2 * results['Precision'] * results['Recall'] \\\n",
    "        / (results['Precision'] + results['Recall'])\n",
    "    \n",
    "    results['Cost Error'] = loss_history[-1]\n",
    "    \n",
    "    results['AUC'] = auc(roc1, roc2)\n",
    "    \n",
    "    results['PR-Score'] = average_precision_score(corrects, predictions)\n",
    "    \n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(roc1, roc2, color='darkorange', \n",
    "             label='ROC curve (area %0.2f)' % results['AUC'])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle = '--')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    pr1, pr2, _ = precision_recall_curve(corrects, predictions)\n",
    "    \n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.step(pr2, pr1, color='black', alpha=0.2,\n",
    "             label='PR curve (area %0.2f)' % results['PR-Score'])\n",
    "    plt.fill_between(pr2, pr1, alpha=0.2, color='black')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate(rdd, b, w, loss_history)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
